---
title: "Econométrie de la finance"
author: "Mohamed Essaied Hamrita"
date: "Octobre 2021"
output:
  pdf_document: 
    includes:
      in_header: pream.tex
    latex_engine: xelatex
    keep_tex: yes
    toc: yes
    number_sections: yes
bibliography: chap3.bib
subtitle: 'Chapitre 3: Les modèles non liléaires univariés'
lang: fr
urlcolor: blue
linkcolor: blue
link-citations: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(icons)
```
```{r, echo=FALSE}
colFmt <- function(x,color) {
  
  outputFormat <- knitr::opts_knit$get("rmarkdown.pandoc.to")
  
  if(outputFormat == 'latex') {
    ret <- paste("\\textcolor{",color,"}{",x,"}",sep="")
  } else if(outputFormat == 'html') {
    ret <- paste("<font color='",color,"'>",x,"</font>",sep="")
  } else {
    ret <- x
  }

  return(ret)
}
```

# Introduction

- Dans ce chapitre nous introduisons quelques modèles non linéaires univariés et nous discutons leurs propriétes statistiques.

- Les modèles introduits sont: le modèle TAR (Threshold AR), le modèle Markov switching (MSM), smooth threshold
autoregressive (STAR) models, et time-varying parameter models (TVM)

# Le modèle TAR

- Le modèle TAR est proposé par [@tong78] et largement utilisé depuis la publication de l'article de [@tong80].

- Nous commençons par un modèle TAR simple à deux régimes, puis nous discutons le modèle TAR à régimes multiples.

- __Définition:__ Une série temporelle $\{X_t\}$ suit un modèle TAR d'ordre $p$ avec la variable seuil $X_{t-d}$, s'il vérifie la relation suivante:
$$
X_t=\begin{cases}
\phi_0+\displaystyle\sum_{i=1}^p\phi_iX_{t-i}+\sigma_1\varepsilon_t \;,\text{ si }\; X_{t-d} \leq r\\
\theta_0+\displaystyle\sum_{i=1}^p\theta_iX_{t-i}+\sigma_2\varepsilon_t \;\;,\text{ si }\; X_{t-d} > r
\end{cases}
$$
avec $\varepsilon_t \stackrel{iid}{\sim}(0,1)$, $\phi_i$ et $\theta_i$ des valeurs réelles telles que $\phi_i \neq \theta_i$ pour quelques valeurs de $i$. $d$ est entier positif et $r$ représente la valeur seuil (threshold).


Le modèle TAR à deux régimes peut s'écrire sous la forme suivante:
$$
X_t=\phi_0+\displaystyle\sum_{i=1}^p\phi_iX_{t-i}+\sigma_1\varepsilon_t+I(X_{t-d} > r)\left(\beta_0+\displaystyle\sum_{i=1}^p\beta_iX_{t-i}+\gamma\varepsilon_t \right)
$$
où $I(X_{t-d} > r)=1$ si $X_{t-d} > r$ et $0$ sinon. $\beta_i=\theta_i-\phi_i$ pour $i=0,1,\ldots,p$ et $\gamma=\sigma_2-\sigma_1$


__Exemple:__  Soit le processus $\{X_t\}$ définit par: 
$$
X_t=\begin{cases}
0.2+0.45X_{t-1}+0.2\varepsilon_t \;,\text{ si }\; X_{t-4} \leq -1\\
0.4+0.6X_{t-1}+0.5\varepsilon_t \;\;,\text{ si }\; X_{t-4} > 1
\end{cases}
$$
```{r tar1, echo=TRUE, comment="", fig.align='center', message=FALSE, fig.width=6, fig.height=4}
library(TSA); 
set.seed(12345)
x=tar.sim(n=500,Phi1=c(0.2,0.45), Phi2=c(0.4,0.6),p=1,
          d=4,sigma1=0.2,sigma2=0.5, thd=-1)$y
plot(x, type="l", col=2, xlab="", ylab=expression(X[t]))
```


## Propriétés statistiques

Considérons le modèle TAR$(1)$ à deux régime:
$$
X_t=\begin{cases}
\phi_1 X_{t-1}+\sigma_1\varepsilon_t \;,\text{ si }\; X_{t-1} \leq 0\\
\theta_1 X_{t-1}+\sigma_2\varepsilon_t \;\;,\text{ si }\; X_{t-1} > 0
\end{cases}
$$
La skeleton de ce modèle est
$$
f(X_{t-1})=\begin{cases}
\phi_1 X_{t-1}\;,\text{ si }\; X_{t-1} \leq 0\\
\theta_1 X_{t-1}\;\;,\text{ si }\; X_{t-1} > 0
\end{cases}
$$
Puisque $x_0$ est supposée un réel quelconque, alors pour que la série $X_t$ soit stable, il faut que $\phi_1 < 1$, $\theta_1 < 1$ et $\phi_1 \theta_1 <1$.

Pour $d>1$, [@chenTsay91] ont montré que les conditions de stabilité du modèle est: $\phi_1 <1$, $\theta_1 <1$ et $\phi_1^{s(d)}\theta_1^{t(d)}$ où $s(d)=t(d)=1$ si $d=1$ et $s(d)=1$, $t(d)=2$ si $d=2$.

## Estimation du modèle TAR

Le modèle TAR à deux régime peut être estimé soit par la méthode du maximum de vraisemblance, soit par la méthode des MCO. Ici, on donne la méthode des MCO. La matrice des variables explicatives peut être écrite de la forme suivante:
$$
X_t(r)=\left(X'_1 \,I(X_{t-d}\leq r)\;\;X'_2\,I(X_{t-d} > r)) \right)
$$
d'où, le modèle peut se réécrire comme:
$$
X_t=X'_t(r)\alpha+\varepsilon_t\;\text{ où }\; \alpha=(\phi'\;\;\theta')'
$$
Pour $r$ donnée, on obtient:
$$
\widehat{\alpha}(r)=\left(X'_t(r)X_t(r)\right)^{-1}X'_t(r)X_t
$$


__Estimation du paramètre__ $r$: L'estimation du vecteur $\alpha$ se fait pour $r$ fixé. Or, $r$ est généralement inconnu. Donc, ce paramètre sera estimé en faisant varier le paramètre $r$, soit une séquence de $n$ valeurs. Pour chaque valeur de $r$, on calcul la somme carré des erreurs du modèle estimé, puis on retient la valeur du $r$ qui correspond à la somme des carrés des erreurs la plus faible.

__Estimation du paramètre__ $d$: Dans la pratique, le paramètre $d$ est inconnu et on doit l'estimer. Ce paramètre sera estimé avec le vecteur $\alpha$ en imposant $d \in \{1,2,\ldots,\overline{d}\}$.

__Test statistique TAR__: 

Une question importante est posée: quand le modèle TAR est statistiquement significative relativement à un modèle linéaire. Il s'agit de tester: $H_0:\; \phi=\theta$. Puisque la valeur du seuil $r$ est inconnue, un tel test devient délicat.

Pour plus amples discussions, voir [@chan90] et [@hansen97]

## Prévision


Pour $d \geq 1$ donné, la prévision à une étape est donnée par:
$$
X_{T+1}=\begin{cases}
\phi_0+\displaystyle\sum_{i=1}^p\phi_iX_{T+1-i},\quad e_{T+1}=\sigma_1\varepsilon_{T+1},\;\;\text{ si } X_{T+1-d}\leq r\\
\theta_0+\displaystyle\sum_{i=1}^p\theta_iX_{T+1-i},\quad e_{T+1}=\sigma_2\varepsilon_{T+1},\;\;\text{ si } X_{T+1-d}> r
\end{cases}
$$

## Application

On veut modéliser le prix du Cooper. On considère la série des prix annuels du Cooper allant de 1800 à 1996 [@hyndman2016]. La série est ajustée après avoir éliminer la tendance. La figure suivante montre l'évolution des prix et la fonction d'auto-corrélation simple.

```{r, echo=TRUE,message=FALSE, fig.align='center', fig.height=4, fig.width=9}
cooper=scan("https://raw.githubusercontent.com/Hamrita/TSFin/main/Chap3/copper.txt")
par(mfrow=c(1,2))
plot(1800:1996,cooper, type="l", col=2, xlab="", ylab="")
acf(cooper)
```

Un premier examen de non linéarité se fait en représentant $x_t$ en fonction de $x_{t-1}$

```{r, echo=TRUE,comment="", fig.align='center', fig.height=4}
y <- cooper[2:197]; x <- cooper[1:196]
m1 <- loess(y~x)  ## local smoothing
sx <- sort(x,index=T)  ## sorting the threshold variable
ix <- sx$ix ## index for order-statistics
 plot(x,y,xlab='x(t-1)',ylab='x(t)')
lines(x[ix],m1$fitted[ix],col="red")
```

On remarque bien que la dépendance entre $x_t$ et $x_{t-1}$ est non linéaire.

__Tests de linéarité__

```{r, echo=TRUE,test, message=FALSE, warning=FALSE, comment=""}
library(nonlinearTseries)
tests=nonlinearityTest(cooper, F)
tests$TarTest
```

On a p-value qui est inférieur à $5\%$, donc on rejette l'hypothèse nulle de linéarité du modèle.

__Estimation__

```{r, est, echo=TRUE, comment="", message=FALSE, warning=FALSE}
library(tsDyn)
mod=setar(cooper,m=3,d=1,mL=2,mH=2)
ss=summary(mod)
ss$coef
ss$AIC
ss$thCoef
```
# Le modèle à changement de régimes Markovien (Markov Switching)

[@hamilton90] introduit _Markov Switching model_  d'ordre $p$, noté par $MS(p)$. Dans le cas de deux régimes, le modèle prend la forme suivante:
$$
X_t=\begin{cases}
\mu_{1,t}+\varepsilon_{1,t}\;\quad \text{ si }s_t = 0,\; t=1,2,\ldots, t_0\\
\mu_{2,t}+\varepsilon_{2,t}\;\quad \text{ si }s_t=1,\; t=t_0+1,t_0+2,\ldots,T
\end{cases}
$$
avec $\mu_{s_t,t}$ est la moyenne conditionnelle aux temps-régimes, et $\varepsilon_{s_t,t}\stackrel{iid}{\sim}iid(0,\sigma_{s_t})$ les innovations conditionnelles aux régimes. La variable état $s_t$ est non observable et on suppose qu'elle est gouvernée par un processus Markovien de premier ordre avec une matrice de transition donnée par:
$$
P=\left( \begin{array}{cc}
p_{00} & p_{01}\\
p_{10} & p_{11}
\end{array}
\right)
$$
avec $p_{ij}=\mathbb{P}(s_t=j|s_{t-1}=i)$ et $p_{00}+p_{01}=p_{10}+p_{11}=1$.


Dans le cas où la moyenne conditionnelle est un processus $AR(1)$, on aura la forme suivante:
$$
X_t=\begin{cases}
\alpha_0+\phi_1 X_{t-1}+\varepsilon_{1,t}\;\quad \text{ si }s_t = 0,\; t=1,2,\ldots, t_0\\
\alpha_1+\phi_2X_{t-1}+\varepsilon_{2,t}\;\quad \text{ si }s_t=1,\; t=t_0+1,t_0+2,\ldots,T
\end{cases}
$$
avec $|\phi_i|<1$, $i=1,2$ et on a $\mathbb{E}(X_t)$ est égale à $\alpha_0/(1-\phi_1)$ si $s_t=0$ et à $\alpha_1/(1-\phi_2)$ si $s_t=1$.

## Simulation d'un modèle MS

Soit le modèle suivant:
$$
X_t=\begin{cases}
0.1+0.4 X_{t-1}+\varepsilon_{1,t}\;\quad \text{ si }s_t = 0,\; \varepsilon_1\sim N(0,0.1)\\
-0.3 -0.5 X_{t-1}+\varepsilon_{2,t}\;\quad \text{ si }s_t=1,\; \varepsilon_2 \sim N(0,0.2)
\end{cases}
$$

```{r,echo=TRUE,sim1, comment="", warning=FALSE, message=FALSE}
St=c(rep(0,60),rep(1,30),rep(0,20),rep(1,50),rep(0,40))
n=length(St); sig1=0.1; sig2=0.2
Xt=numeric(n+1)
set.seed(123)
Xt[1]=rnorm(1);e1=rnorm(n,0,sig1); e2=rnorm(n,0,sig2)
for(i in 2:(n+1)){
  if(St[i-1]==0){
    Xt[i]=0.1+0.4*Xt[i-1]+e1[i]
  }else{
    Xt[i]=-0.3-0.5*Xt[i-1]+e2[i]
  }
}
xt=Xt[-1]
plot.ts(xt)
```

## Estimation

Les paramètres intéressés dans le modèle $MS(2)$ avec moyenne conditionnelle suivant un processus $AR(1)$ sont représentés par le vecteur $\theta=(\mu_0,\mu_1,\sigma_1,\sigma_2,p_{00},p_{11})$ dans le cas où la moyenne conditionnelle est supposée constante.

De plus, on suppose que l'inférence de la variable $s_t$ pourrait prendre la forme suivante:
$$
P_{j,t}=\mathbb{P}(s_t=j|\mathcal{I}_t;\theta)
$$
où $\mathcal{I}_t$ représente l'information disponible à l'instant $t$ et $i=0,1$. Puis, on déduit la densité conditionnelle de la variable $X_t$ qui est donnée par
$$
f(X_t|s_t=i, \mathcal{I}_{t-1},\theta)=\dfrac{1}{\sigma_i\sqrt{2\pi}}\exp\left(-\dfrac{1}{2\sigma_i^2}(X_t-\mu_i)^2\right),\;\;i=0,1
$$
[@hamilton90] a montré que la densité conditionnelle de la $t$ième observation est
$$
f(X_t| \mathcal{I}_{t-1},\theta)=\sum_{i=0}^1\sum_{j=0}^1p_{ij}P_{j,t}f(X_t|s_t=i, \mathcal{I}_{t-1},\theta)
$$
On en déduit la fonction de vraisemblance
$$
l=\sum_{t=1}^T\log\Big(f(X_t| \mathcal{I}_{t-1},\theta)\Big)
$$
__Remarques:__

1) Il y a d'autres méthodes itératives d'estimation telles que la méthode de Markov chain Monte Carlo (MCMC).

2) Les erreurs peuvent avoir une variance qui varie conditionnellement  en fonction du temps, dans ce cas on parle du modèle GARCH à changement de régimes Markovien.

3) Dans ce cours, nous avons présenté l'estimation du modèle le plus simple (à changement de deux régimes), mais dans la pratique, il y a plus que deux régimes et le nombre de changements est généralement est inconnu et, alors doit être estimé.

## Exemples

### Exemple 1

On commence par l'estimation du modèle simulé ci-dessus.

```{r, echo=TRUE, message=FALSE, warning=FALSE, comment=""}
dat=data.frame(cbind(xt[-n],xt[-1]))
names(dat)=c("y","x")
lm1=lm(y~x, data=dat)
library(MSwM)
ms1=msmFit(lm1,k=2,sw=c(T,T,T))
summary(ms1)
```

### Exemple 2
Dans l'exemple suivant, on veut étudier la relation entre le nombre de décès des accidents et les conditions climatiques.

Les données du `traffic` contiennent le nombre quotidien de décès dans des accidents de la circulation en Espagne au cours de l'année 2010, la température moyenne quotidienne et la somme quotidienne des précipitations.

```{r, echo=TRUE, warning=FALSE, message=FALSE, comment=""}
data(traffic)
head(traffic)
```
Dans cet exemple, la variable de réponse est une variable de comptage. Pour cette raison, nous ajustons un modèle linéaire généralisé de Poisson.

```{r, echo=TRUE, message=FALSE, warning=FALSE, comment=""}
mod1=glm(NDead~Temp+Prec, family="poisson",data=traffic)
m1=msmFit(mod1,k=2,sw=c(TRUE,TRUE,TRUE),family="poisson",
          control=list(parallel=FALSE))
slotNames(m1)
summary(m1)
```
```{r, echo=TRUE, message=FALSE, warning=FALSE, comment=""}
filtProb=m1@Fit@filtProb
smoothProb=m1@Fit@smoProb
par(mfrow=c(2,2))
plot(filtProb[,1], type="l", ylab="Prob Filt 1")
plot(filtProb[,2], type="l", ylab="Prob Filt 2")
plot(smoothProb[,1], type="l", ylab="Prob lisse 1")
plot(smoothProb[,1], type="l", ylab="Prob lisse 2")
```

# Modèle à coefficients dynamiques (Time varying coefficients model)

__Références__