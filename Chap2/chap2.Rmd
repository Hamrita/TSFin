---
title: 'Econométrie de la finance'
subtitle: 'Chapitre 2: Les modèles GARCH'
author: "Mohamed Essaied Hamrita"
date: "Octobre 2021"
bibliography: chap2.bib
output:
  ioslides_presentation:
    incremental: yes
    widescreen: yes
    smaller: yes
    #css: styles.css
---
<style type="text/css">
body p {
  color: #000000;
}
slides > slide.title-slide hgroup h1 {
  font-weight: bold;
  font-size: 26pt;
  color: red;
  position: fixed;
  top: 30%;
  left: 50%;
  transform: translate(-50%, -50%);
}
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(icons)
```
```{r, echo=FALSE}
colFmt <- function(x,color) {
  
  outputFormat <- knitr::opts_knit$get("rmarkdown.pandoc.to")
  
  if(outputFormat == 'latex') {
    ret <- paste("\\textcolor{",color,"}{",x,"}",sep="")
  } else if(outputFormat == 'html') {
    ret <- paste("<font color='",color,"'>",x,"</font>",sep="")
  } else {
    ret <- x
  }

  return(ret)
}
```

## Introduction

- Comme nous l'avons déjà vu dans le chapitre précédent, que dans la plus part des cas, les séries financières remettent en cause la propriété d'homoscédasticité. 

- L'approche ARCH/GARCH est proposée pour prendre en compte des variances conditionnelles dépendantes du temps.

## Le modèle ARCH

__Définition:__ Soit le processus $X_t=\{X_1, X_2, \ldots, X_T\}$ et $\mathcal{I}_{t-1}=\{X_1, X_2, \ldots, X_{t-1}\}$ l'information disponible à l'instant $t-1$. $X_t$ est dit un processus `r colFmt("__ARCH__", "red")` d'ordre $p$, noté $X_t \sim ARCH(p)$, s'il vérifie la relation suivante:
$$
\begin{cases}
X_t=\varepsilon_t, \;\; \varepsilon \sim N(0,\sigma_t)\;\;\qquad\qquad\qquad\quad\;\:\: \text{ (Mean conditional equation)}\\
\varepsilon_t=Z_t\sigma_t,\;\; Z_t \stackrel{iid}\sim N(0,1)\\
\sigma^2_t=a_0 + a_1 \varepsilon^2_{t-1}+a_2 \varepsilon^2_{t-2} + \ldots +a_p \varepsilon^2_{t-p}\;\;\text{ (Variance conditional equation)}
\end{cases}
$$
avec $a_0 >0$, $a_1,\ldots, a_p \geq 0$ et $a_1+a_2+\ldots + a_p<1$.

$\sigma^2_t$ est la __variance conditionnelle__ du processus $X_t$, $\sigma^2_t=\mathbb{V}\left(X_t|\mathcal{I}_{t-1}\right)$.

Ce processus est proposé par [@engle82].

---

__Exemple:__  Soit $X_t \sim ARCH(1)$. La figure suivante est une réalisation (simulation) du modèle $ARCH(1)$ définit par: $X_t=\varepsilon_t=\sigma_t Z_t$ et $\sigma^2_t=0.4+0.7\varepsilon^2_{t-1}$.

```{r arch1Sim, echo=TRUE, comment="", fig.align='center', message=FALSE}
library(fGarch); set.seed(12345)
arch1=garchSim(garchSpec(model=list(omega=0.4,alpha=0.7, beta=0)), n=300, extended = T)
par(mfrow=c(1,2))
plot(arch1$garch, type="l", col=2, main="Simulation ARCH(1)", xlab="", ylab="")
plot(arch1$sigma^2, type="l", col=2, main="Variance conditionnelle", xlab="", ylab="")
```


## Propriétés statistiques

Soit $X_t \sim ARCH(p)$. On a pour tout $t$ et $h \geq 1$,

- $\mathbb{E}\left(X_t|\mathcal{I}_{t-1} \right)=0$ et $\mathbb{E}\left(X_t \right)=0$.

- $\mathbb{V}\left(X_t|\mathcal{I}_{t-1} \right)= \sigma^2_t, \; \forall t$ et $\mathbb{V}\left(X_t \right)=\dfrac{a_0}{1-\displaystyle \sum_{i=1}^pa_i}$.

- $\mathbb{Cov}\left(X_t X_{t+h}|\mathcal{I}_{t-1} \right)=0$ pour $h \geq 1$ et $\mathbb{Cov}\left(X_t X_{t+h}\right)=0$.

- __Rappel__

- $\mathbb{E}(X)=\mathbb{E}\big(\mathbb{E}(X|Y)\big)$.

- Soit $A_1 \subseteq A_2$, $\mathbb{E}(X|A_1)=\mathbb{E}\big(\mathbb{E}(X|A_2)|A_1\big)$.

## La construction du modèle ARCH

- La construction du modèle ARCH passe par 4 étapes:

   1. Détermination de l'ordre $p$.
   
   2. Estimation du modèle $ARCH(p)$.
   
   3. Diagnostic du modèle estimé.
   
   4. Prévision.


---

__Détermination de l'ordre__ $\mathbf{p}$__:__

- Similairement aux modèles ARMA, l'ordre $p$ peut être déterminé en examinant la fonction d'auto-corrélation partielle de $\varepsilon_t^2$.

- On peut aussi faire recours aux critères de sélection: AIC, SIC et AICc.

- $AIC=-2logL+2k$, (Akaike information criteria) $k=$ nombre de paramètres dans le modèle estimé.

- $BIC=-2logL+\log(T)k$. (Bayesian information criteria )

- Pour un échantillon de petite taille, on utilise le critère $AICc$ qui est défini par
$$
AICc=AIC+\dfrac{2k(k+1)}{T-k-1}
$$

---

Reprenons notre exemple du chapitre précédent, les rendements du bitcoin.


```{r Rbtc, comment=NA, warning=FALSE, message=FALSE, echo=TRUE, fig.align='center', out.width="80%",fig.height=4}
library(quantmod); library(zoo)
btc=getSymbols("BTC-USD", src="yahoo", from="2014-09-17",to="2021-10-20",auto.assign = FALSE)
closedAdj=zoo(na.omit(btc[,6])); rt=diff(log(closedAdj))
pacf(rt^2,na.action = na.pass, col=4, xlab="")
```

---

D'après le graphique de la fonction d'auto-corrélation partielle de $r_t^2$, on remarque bien que le modèle ARCH d'ordre 1 est approprié aux rendements du BTC. On remarque aussi, que les pacf d'ordres 4 et 7 sont aussi significativement différents de zéro.

Déterminons les valeurs des critères d'information pour les modèles ARCH(1) et ARCH(4). (Lors de la modélisation GARCH, on ne dépasse pas l'ordre 5).

Sous R, il existe plusieurs packages permettant l'estimation des modèles GARCH tels que `tseries`, `fGarch`, `rugarch`.

Ici, nous décrivons l'utilisation du package `rugarch`. Pour l'estimation du modèle ARCH, nous devons spécifier le modèle à estimer en donnant les ordres des différents modèles (mean and variance equations).

La fonction est `ugarchspec` et prend comme arguments principaux `variance.model` et `mean.model`. Ces deux arguments sont des listes. 

---

```{r crit, echo=TRUE, comment="", message=FALSE}
library(rugarch)   # charger le package
spec1=ugarchspec(variance.model = list(garchOrder=c(1,0)),
                mean.model = list(armaOrder=c(0,0)))
spec4=ugarchspec(variance.model = list(garchOrder=c(4,0)),
                mean.model = list(armaOrder=c(0,0)))
fit1=ugarchfit(spec1,rt)  # Estimation
fit4=ugarchfit(spec4,rt)
t(infocriteria(fit1))  # critères d'information (normalisés)
t(infocriteria(fit4))
```



---

__Estimation__

- Sous l'hypothèse de normalité des erreurs, la fonction de vraisemblance d'un modèle $ARCH(p)$ est:
$$
L(\varepsilon_1, \varepsilon_2,\ldots,\varepsilon_T|\mathbf{a})=\prod_{i=p+1}^T\dfrac{1}{\sqrt{2\pi\sigma^2_t}}\exp{\left(-\dfrac{\varepsilon_t^2}{2\sigma^2_t} \right)}\times f(\varepsilon_1, \varepsilon_2,\ldots,\varepsilon_T|\mathbf{a})
$$
où $\mathbf{a}=(a_0,a_1,\ldots,a_p)$ et $f(\varepsilon_1, \varepsilon_2,\ldots,\varepsilon_T|\mathbf{a})$ la densité conjointe conditionnelle des erreurs.

- Maximiser la fonction de vraisemblance conditionnelle est équivalent à maximiser son logarithme. Le logarithme de la vraisemblance conditionnelle est:
$$
\ell(\varepsilon_{\color{red}{p+1}}, \varepsilon_\color{red}{{p+2}},\ldots,\varepsilon_\color{red}{T}|\mathbf{a}, a_\color{red}{1},a_\color{red}{2},\ldots,a_\color{red}{p})=\sum_{i=p+1}^T\left[-\frac{1}{2}\log(2\pi)-\frac{1}{2}\log(\sigma^2_t)-\frac{1}{2}\frac{\varepsilon_t^2}{\sigma^2_t} \right]
$$
où $\sigma^2_t=a_0+a_1\varepsilon^2_{t-1}+a_2\varepsilon^2_{t-2}+\ldots+a_p\varepsilon^2_{t-p}$ qui peut être calculé récursivement.

- __Remarque:__ On peut aussi utiliser d'autres distributions autre que la loi normale telles que la loi de student ou la loi GED (Generalized Error Distribution).

---

__Exemple:__ Soit $X_t \sim ARCH(1)$. Donner les estimateurs de $\mu$, $a_0$, et $a_1$ par la méthode de MV.

- Le logarithme de la vraisemblance conditionnelle est donnée par:
$$
\ell(\varepsilon_{2}, \varepsilon_{3},\ldots,\varepsilon_T|\mathbf{a}, a_1,a_2)=\sum_{i=2}^T\left[-\frac{1}{2}\log(2\pi)-\frac{1}{2}\log(a_0+a_1X^2_{t-1})-\frac{1}{2}\frac{(X_t-\mu)^2}{a_0+a_1X^2_{t-1}} \right]
$$
Les paramètres $\mu$, $a_0$ et $a_1$ se déduisent en résolvant le système suivant
$$
\begin{cases}
\frac{\partial \ell}{\partial \mu}=0 \\
\frac{\partial \ell}{\partial a_0}=0 \\
\frac{\partial \ell}{\partial a_1}=0 
\end{cases}
$$

---

```{r estArch, echo=TRUE, comment=""}
fit1@fit$matcoef
show(fit1)
```

---

- Estimation avec des erreurs de loi de student:

- Tout d'abord, examinons la distribution des erreurs et la comparons par la densité normale.

```{r hist_rt, echo=TRUE, comment="",fig.align='center', out.width="80%",fig.height=5, eval=FALSE}
hist(rt,col=4, prob=T, breaks = 50) # histogramme or rt
lines(density(rt), col=2,lwd=3) # density estimation (kernel)
curve(dnorm(x, mean(rt),sd(rt)), lwd=3, col="darkorchid3", add=T) # normal density
legend("topleft",bty="n",lwd=3, col=c(2,"darkorchid3"),
       legend=c("Kernel","Normal"))

```

---

```{r hist_rt2, echo=FALSE, comment="",fig.align='center', out.width="80%",fig.height=6}
hist(rt,col=4, prob=T, breaks = 50) # histogramme or rt
lines(density(rt), col=2,lwd=3) # density estimation (kernel)
curve(dnorm(x, mean(rt),sd(rt)), lwd=3, col="darkorchid3", add=T) # normal density
legend("topleft",bty="n",lwd=3, col=c(2,"darkorchid3"),
       legend=c("Kernel","Normal"))

```

---

```{r archStd, echo=TRUE, comment="", message=FALSE}
specSt=ugarchspec(list(garchOrder=c(1,0)), list(armaOrder=c(0,0)),distribution.model = "std")
fitst=ugarchfit(specSt,rt)
show(fitst)
```


---

- Pour un modèle ARCH bien approprié, les erreurs standards $\widetilde{\varepsilon}_t=\frac{\varepsilon_t}{\sigma_t}$ doivent être $iid$. La statistique de Ljung-Box peut être appliqué sur les erreurs standards ($\widetilde{\varepsilon}_t$) pour examiner l'équation de la moyenne conditionnelle et sur leurs carrés ($\widetilde{\varepsilon}_t^2$) pour examiner l'équation de la variance conditionnelle. 

- Le package `rugarch` utilise plutôt les statistiques de Ljung-Box pondérée et LM-ARCH pondérée proposé par [@Fisher12] 



## Propriétés des séries financières

- `r colFmt("grande variété","red")` des séries utilisées (prix d'action, taux d'intérêt, taux de change etc.), importance de la fréquence d'observation (seconde, minute, heure, jour, etc), disponibilité d'échantillons de très grande taille.

- existence de régularités statistiques ("`r colFmt("faits stylisés","red")`") communes à un très grand nombre de séries financières et difficiles à reproduire artificiellement à partir de modèles stochastiques. @mandelbrot69.

   - Non stationnarité des prix $p_t$
   - Possible stationnarité des rendements.
   - Regroupement des extrêmes (volatility clustering): On observe empiriquement que de fortes variations des rendements sont généralement suivies de fortes variations. Ce type de phénomène remet en cause l'hypothèse d'homoscédasticité.
   - Non corrélation des rendements mais auto-corrélation des carrés.
   - Queues de distribution épaisses: la distribution des résidus demeure leptokurtique.
   - Asymétrie: Les baisses du cours génèrent plus de volatilité que les hausses de même amplitude (effet de levier).
   - Saisonnalité: Les rendements présentent de nombreux phénomènes de saisonnalité (effets week end, effet janvier, etc..)

## Tests de linéarité 

- La flexibilité des modèles non linéaires dans l'ajustement des données peut rencontrer le problème de trouver une structure fallacieuse (spurious) dans une série chronologique donnée.

- Il est donc important de vérifier la nécessité d'utiliser des modèles non linéaires. À cette fin, nous introduisons des tests de linéarité pour les données de séries temporelles. Les tests `r colFmt("__paramétriques__","red")` et `r colFmt("__non paramétriques__","red")` sont pris en compte.

- Tests paramétriques:
   - Le test de multiplicateur de Lagrange ( @engle82 ) 
   - Le test RESET (Regression error specification test) (  @Ramsey69, @kenan85  )

- Tests non paramétriques
    - Le test McLeod-Li ( @McLi83 ) 
    - Le test BDS ( @bds )
    - Test de Peña-Rodríguez [@pena02; -@Pena2006]
    
---

Tests paramétriques

`r colFmt("__Le test LM:__","blue")` Il s'agit de tester l'effet ARCH (héteroscédasticité conditionnelle) proposé par @engle82. L'hypothèse nulle est: $H_0: \alpha_1 = \alpha_2 = \ldots = \alpha_m = 0$ du modèle: $e_t^2=\alpha_0+\alpha_1 e_{t-1}^2 + \alpha_2 e_{t-2}^2 + \ldots = \alpha_m e_{t-m}^2 + a_t$, $t=1,2,\ldots, T$.

$e_t$ est la série des résidus obtenue suite à un modèle linéaire (ARIMA, régression linéaire).

$$LM=T R^2 \stackrel{H_0}{\sim}\chi^2(m) \text{ où }R^2 \text{ est le coefficient de détermination.} $$

Si $LM < \chi^2_{\alpha}(m)$, l'hypothèse nulle est acceptée; __absence d'effet ARCH__.

__Remarques:__ 

- Ce test est équivalent au test de Fisher;
$$
F=\dfrac{(SCR_0-SCR_1)/m}{SCR_1/(T-2m-1)}\stackrel{H_0}{\sim}F(m,T-2m-1)
$$
où $SCR_0$ et $SCR_1$ sont les sommes des carrés résiduelles sous $H_0$ et $H_1$ respectivement.

- On accepte l'hypothèse nulle si p.value est supérieure à $\alpha \%$.

---

```{r archtest, echo=TRUE}
archLM=function(x,p=1, disp=T){
  # x: time series;  p: lag order; disp: display or not the result
  mat=embed(x^2,(p+1))
  reg=summary(lm(mat[,1]~mat[,-1]))
  r2=reg$r.squared
  statistic=r2*length(resid(reg))
  cv=qchisq(0.95,p)
  pvalue=1-pchisq(statistic,p)
  if(disp){
  cat("================================================\n")
  cat("=======           ARCH LM test         =========\n")
  cat("================================================\n")
  cat("====   H0: no ARCH effects             =========\n\n")
  
  cat(" Chi-squared: ", statistic, "  df: ", p,  "  p.value: ", pvalue, 
      "  critical value 5%: ", cv, " \n")
  cat(" F-statistic", reg$fstatistic[1]," df:  ", reg$fstatistic[-1],"p.value: ",
      1-pf(reg$fstatistic[1], reg$fstatistic[2],reg$fstatistic[3]), 
      " critical value 5%: ", qf(0.95,reg$fstatistic[2], reg$fstatistic[3]), "\n")
  }
  return(invisible(list("Chi-squared"=statistic, "p-value"=pvalue,
              "df"=p)))
}
```

---

Appliquons le test LM sur les rendements du Bitcoin (en supposant que $r_t=m+\epsilon_t$)


Le résultat montre bien __l'existence d'un effet ARCH__ sur les rendements du Bitcoin (p.value presque nulle).

---

`r colFmt("__RESET Test__","blue")`

@Ramsey69 a proposé un test de spécification pour les modèles de régression linéaire. Ce test est aussi applicable aux modèles $AR$. Les étapes du test sont décrites comme suit:

- Estimation, par MCO, le modèle: $x_t=\phi_0+\phi_1 x_{t-1}+\ldots + \phi_p x_{t-p}+\varepsilon_t$, on obtient, alors $\mathbf{\widehat{\phi}}=(\widehat{\phi}_0,\widehat{\phi}_1,\ldots, \widehat{\phi}_p)$, $\widehat{x}_t$, $e_t=\widehat{\varepsilon}_t=x_t-\widehat{x}_t$ et $SCR_0=\sum e_t^2$.
- Estimation, par MCO, le modèle: $x_t=\alpha_0+\alpha_1 x_{t-1}+\ldots + \alpha_p x_{t-p}+ \beta_{\color{red}{1}} \widehat{x}_t^{\color{red}{2}}+\beta_{\color{red}{2}} \widehat{x}_t^{\color{red}{3}}+ \ldots +\beta_{\color{red}{s}} \widehat{x}_t^{\color{red}{s+1}}+\nu_t$, pour $s \geq 1$. En déduire $SCR_1=\sum \widehat{\nu}_t^2$.
Le modèle est non linéaire, si $H_0: \alpha_0=\alpha_1=\ldots=\beta_1=\beta_s=0$, donc on peut faire recours au test du Fisher.
- Calcul du statistique $F=\dfrac{(SCR_0-SCR_1)/(s+p+1)}{SCR_1/(T-2p-s-1)}\stackrel{H_0}{\sim}F(s+p+1,T-2p-s-1)$

- `r colFmt("__Remarque:__","red")`
Parce que les variables $\widehat{x}_j$ pour $j=2, \ldots, s+1$ ont tendance à être fortement corrolées avec $(x_{t-1},x_{t-2}, \ldots, x_{t-p})$ et entre elles, les composantes principales de $(\widehat{x}_t^2, \ldots, \widehat{x}_t^{s+1})$ qui ne sont pas colinéaires avec $(x_{t-1},x_{t-2}, \ldots, x_{t-p})$ sont souvent utilisées dans l'ajustement de la deuxième équation.

---

```{r resetTest, echo=TRUE, comment=""}
reset.test=function(x,p=1,s=1){
  # Estimate of AR(p)
  xx=embed(x,(p+1))
  mod1=lm(xx[,1]~xx[,-1])
  scr0=sum(mod1$residuals^2)
  xx.hat=mod1$fitted.values
  et=mod1$residuals;  TT=length(xx.hat)
  # Estimate of model 2
  xxx=matrix(0,nr=TT, nc=s)
  for(ii in 1:s)  xxx[,ii]=xx.hat^(s+1)
  X=cbind(xx[,-1],xxx)
  mod2=lm(et~X);  scr1=sum(mod2$residuals^2)
  df2=TT-p-s-1 ; df1=s+p+1
  F_stat=df2/df1*(scr0-scr1)/scr1
  pval=pf(F_stat,df1,df2,lower.tail = F)
  cval=qf(0.95,df1,df2)
  dec=ifelse(pval < 0.05, "H0 is rejected", "H0 is accepted")
  cat("=======================================\n")
  cat("        RESET test                     \n")
  cat("=======================================\n\n")
  cat("      H0: the model is linear          \n\n")
  cat("F-statistic: ", F_stat, "df: ", c(df1,df2), " p-value: ", pval, "CV 5%: ", cval,"\n")
  cat("Decision: ", dec , "\n")
  
  return(invisible(list(Fstatistic=F_stat,df=c(df1,df2),p.value=pval)))
}
```

---



---

`r colFmt("__Keenan Test__","blue")`

- @kenan85 a proposé un test de non-linéarité pour les séries temporelles qui utilise uniquement $\widehat{x}^2_t$ et modifie la deuxième étape du test RESET pour éviter la multi-colinéarité entre $\widehat{x}^2_t$ et $(x_{t-1},x_{t-2}, \ldots, x_{t-p})$. 
- Plus précisément, la deuxième régression linéaire est divisée en deux étapes. En premier lieu, on supprime la dépendance linéaire de $\widehat{x}^2_t$ sur $(x_{t-1},x_{t-2}, \ldots, x_{t-p})$ en ajustant la régression $\widehat{x}^2_t=\alpha_0+\alpha_1 x_{t-1}+\ldots+\alpha_p x_{t-p}+v_t$ et déduire les résidus $\widehat{v}_t$.
- En second lieu, on considère le modèle $e_t = \gamma \widehat{v}_t + u_t$ pour obtenir la somme des carrées $SCR_0=\sum\widehat{u}^2_t$ afin de tester l'hypothèse nulle $H0: \gamma =0$

---


```{r kenTest, echo=TRUE, comment=""}
keenan.test=function(x,p=1){
  # Estimate of AR(p)
  xx=embed(x,(p+1))
  mod1=lm(xx[,1]~xx[,-1])
  xx.hat=mod1$fitted.values
  et=mod1$residuals
  # Estimate of model 2
  # (i)
  mod21=lm(xx.hat^2~xx[,-1])
  vt=mod21$residuals
  mod22=lm(et ~ -1+vt)
  F_stat=summary(mod22)$fstatistic[1]
  df=summary(mod22)$fstatistic[-1]
  pval=pf(F_stat,df[1],df[2],lower.tail = F)
  cval=qf(0.95,df[1],df[2])
  dec=ifelse(pval < 0.05, "H0 is rejected", "H0 is accepted")
  cat("=======================================\n")
  cat("        Keenan test                    \n")
  cat("=======================================\n")
  cat("      H0: the model is linear          \n\n")
  print(summary(mod22)$coefficients)
  cat("\n\nF-statistic: ", F_stat, "df: ", c(df), " p-value: ", pval, "CV 5%: ", cval,"\n")
  cat("Decision: ", dec , "\n")
  
  return(invisible(list(Fstatistic=F_stat,df=c(df),p.value=pval)))
}
```

---


---

`r colFmt("__Le test de McLeod-Li __","blue")` 

- Une statistique de type test-portemanteau, basée sur la fonction d'auto-corrélation des carrés résiduels obtenus à partir d'un modèle ARMA, a été proposée par @McLi83. L'idée est d'appliquer la statistique de Ljung-Box aux carrés des résidus d'un modèle $ARMA(p,q)$ pour vérifier l'inadéquation du modèle. Par conséquent, la statistique de test est
$$
Q(m)=n(n+2)\sum_{i=1}^n\dfrac{\widehat{\rho}^2_i(e^2_i)}{n-i}
$$
où $n$ est la taille de l'échantillon, $m$ est un nombre correctement choisi d'auto-corrélations utilisées dans le test et $e_i$ les résidus du modèle ARMA.
- Sous $H_0$, $Q(m) \sim \chi^2(m-p-q)$.

---

---

- `r colFmt("__Le test BDS :__","blue")` Le test BDS (@bds), développé dans le cadre de la théorie du chaos, est l'un des tests de non-linéarité les plus populaires.
- La statistique BDS est basée sur l'intégrale des corrélations. Etant donné une série temporelle $\{x_t\}$, et en posant $x_t^m=\{x_t, x_{t-1},\ldots,x_{t-m+1}\}$, l'intégrale des corrélations est donnée par:
$$
C_{m,T}(\epsilon)=\sum_{t < s}I_{\epsilon}x^m_t x^s_t \left\{\dfrac{2}{T_m(T_m-1)} \right\}
$$
où $T_m=T−(m−1)$ et $I_{\epsilon}x^m_tx^m_s$ est une fonction indicatrice égale à un si $||x^m_t-x^s_t|| < \epsilon$ et égale à zéro sinon avec $||.||$ désigne la norme supérieure.
- La statistique de BDS est donnée par:
$$
W_{m \epsilon}=\sqrt{T}\dfrac{C_{m,T}(\epsilon)-C_{1,T}(\epsilon)^m}{s_{m,T}}
$$
où $s_{m,T}$ est l'écart-type estimé.
- Sous l'hypothèse nulle, pour laquelle la série $\{x_t\}$ est considérée $iid$, @bds ont montré que $W_{m \epsilon}$ converge vers la loi normale standard.
- Dans la pratique, le test BDS est appliqué aux résidus suite à une estimation d'un modèle linéaire.

---

---

`r colFmt("__Test de Peña-Rodríguez__","blue")` [@pena02; -@Pena2006]

- Peña et Rodríguez (2002) ont proposé une statistique de test de Portemanteau qui peut être utilisée pour la vérification d'un modèle linéairement ajusté, y compris la non-linéarité dans les résidus.
- Soit $e_t$ les résidus suite à un modèle linéaire. Soit $z_t$ une fonction de $e_t$; $z_t=e^2_t$ ou $z_t=|e_t|$. L'auto-corrélation empirique de retard $k$ de $z_t$ est:
$$
\widehat{\rho}_k=\dfrac{\displaystyle{\sum_{i=k+1}^T(z_{t-i}}-\overline{z})(z_t-\overline{z})}{\displaystyle{\sum_{i=1}^T}(z_t-\overline{z})^2}
$$
- Pour un entier positif donné, $m$, l'hypothèse nulle du test de Peña-Rodríguez est $H_0: \rho=\rho_2=\ldots=\rho_m$. La statistique est $\widehat{D}_m=T\left(1-\left|\widehat{R}_m \right|^{1/m} \right)$ où
$$
\widehat{R}_m=\left(\begin{array}{cccc}
1 & \widehat{\rho}_1 & \ldots & \widehat{\rho}_m\\
\widehat{\rho}_1 & 1 & \ldots & \widehat{\rho}_{m-1}\\
\cdots & \cdots & \ddots & \vdots\\
\widehat{\rho}_m & \widehat{\rho}_{m-1} & \ldots & 1
\end{array}\right)
$$

---

- En utilisant l'idée de pseudo-vraisemblance, [@Pena2006] ont modifié la statistique du test:
$$
\widehat{D}^*_m=-\dfrac{T}{m+1}\log\left(|\widehat{R}| \right)
$$
qui est distribuée asymptotiquement comme un mélange de $m$ variables aléatoires indépendantes $\chi^2(1)$. Cependant, les auteurs ont dérivé deux approximations pour simplifier le calcul. Ils ont aboutit au résultat $N\widehat{D}^*_m \sim N(0,1)$ où $N\widehat{D}^*_m$ est une approximation à $\widehat{D}^*_m$ (Eq 10 de @Pena2006).

- Sous `r fontawesome::fa("r-project", "steelblue", height="1em")`, on peut faire recours à la fonction `PRnd` du package `NTS`.


---
__Références__

